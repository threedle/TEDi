<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="TEDi Motion Diffusion, entangling time-axis of diffusion and time-axis of animation for motion synthesis">
  <meta name="keywords" content="TEDi, Motion Diffusion Models, Long-term Motion Synthesis">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TEDi Motion</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://3dl.cs.uchicago.edu/">
          <span class="threedle-icon"></span>
        </a>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><span style="text-shadow: 2px 2px 0px #c0c0c0">TEDi:
                <strong>T</strong>emporally-Entangled Diffusion for Long-Term Motion Synthesis</span></h1>
            <!-- <div class="column has-text-centered">
              <p>Paper and code links soon!</p>
            </div> -->
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Code Link. -->
            <span class="link-block">
                  <a href="https://github.com/threedle/TEDi" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video width="1280" height="960" poster="" id="bunny-headphones" autoplay muted loop playsinline
          height="100%">
          <source src="./static/videos/teaser_animation.mov" type="video/mp4">
        </video>
        <div class="content has-text-justified">
          <p>
            We present <strong>TEDi</strong> (<strong>T</strong>emporally-<strong>E</strong>ntangled <strong>Di</strong>ffusion), a motion diffusion framework that
            <strong>entangles</strong> the time-axis of diffusion and the temporal-axis of motion. Compared to typical motion diffusion models, which require 
            several hundreds of denoising steps to produce a short sequence of clean motion from noise (left), TEDi generates <strong>long-term sequences</strong>
            of clean frames throughout the process after being initiated by a motion primer - a short motion sequence (right).
          </p>
        </div>
      </div>
    </div>
  </section>


  <section class="section">

    <div class="container is-max-desktop">
      <div class="bg">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">

              <p>
                The gradual nature of a diffusion process that synthesizes samples in small
                increments constitutes a key ingredient of Denoising Diffusion Probabilistic
                Models (DDPM), which have presented unprecedented quality in image
                synthesis and has been recently explored in the motion domain. In this work, we
                propose to adapt the gradual diffusion concept (operating along a diffusion
                time-axis) into the temporal-axis of the motion sequence. Our key idea is
                to extend the DDPM framework to support temporally varying denoising,
                thereby entangling the two axes. Using our special formulation, we itera-
                tively denoise a motion buffer that contains a set of increasingly-noised poses,
                which auto-regressively produces an arbitrarily long stream of frames. With
                a stationary diffusion time-axis, in each diffusion step we increment only
                the temporal-axis of the motion such that the framework produces a new,
                clean frame which is removed from the beginning of the buffer, followed
                by a newly drawn noise vector that is appended to it. This new mechanism
                paves the way toward a new framework for long-term motion synthesis
                with applications to character animation and other domains.
              </p>
            </div>
          </div>
        </div>
      </div>
      <hr class="divider" />
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Key Idea</h2>

          <div class="overview-image">
            <video poster="" id="bunny-headphones" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/website_entangle.mp4" type="video/mp4">
            </video>
          </div>
          <div class="content has-text-justified ">
            <p>
              We entangle the time-axis of diffusion and the temporal-axis of motion, resulting in a 
              <strong><em>motion buffer</em></strong> which encodes noisy predictions of future frames.
            </p>
          </div>
        </div>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Training</h2>
          <div class="content has-text-justified">
          </div>
          <div class="overview-image">
            <img src="./static/images/figures/training.png" type="image/png">
          </div>
          <div class="content has-text-justified">
            <p>
              We train our diffusion-based model to remove temporally-varying noise that is applied to clean sequences
              during training. In each iteration, we fetch a motion sequence from the dataset, apply noise to it
              according to a noise level schedule, and train our network to predict the clean motion sequence in a
              supervised fashion.
            </p>
          </div>
          <h2 class="title is-3">Inference</h2>
          <div class="overview-image">
            <img src="./static/images/figures/typewriter-2.png" type="image/png">
          </div>
          <div class="content has-text-justified">
            <p>
              Our method is capable of generating an arbitrarily long motion sequence. First, we initialize our
              <i>motion buffer</i> with a set of increasingly-noised motion frames. Then (step 1) we denoise the
              entire motion buffer, (step 2) pop the new, clean frame at the beginning of the motion buffer, and then
              (step 3) push noise into the end of the motion buffer. This process is repeated recursively.
            </p>
          </div>
        </div>
      </div>

      <hr class="divider" />
      <div class="columns is-centered has-text-centered bg">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Long-Term Generation</h2>
          <div class="content has-text-justified ">
            <p>
              Our method synthesizes arbitrarily long motion sequences.
            </p>
          </div>
          <div class="overview-image">
            <video poster="" id="bunny-headphones" controls autoplay muted loop playsinline height="100%">
              <source src="./static/videos/website_long.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <hr class="divider" />

      <section class="hero">

        <div class="hero-body">
          <div class="columns is-centered has-text-centered">
            <h2 class="title is-3 is-centered">Gallery</h2>
          </div>
          <div class="content has-text-justified">
            <p> Our framwork generates diverse motion sequences with the same model.</p>
          </div>
          
          <div class="container">
            <div id="results-carousel" class="results-carousel">
              <div class="item item-bunny-headphones">
                <video poster="" id="bunny-headphones" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/website_gallery_1.mov" type="video/mp4">
                </video>
                <p class="has-text-centered">Wandering around</p>
              </div>
              <div class="item item-bunny-headphones">
                <video poster="" id="bunny-headphones" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/website_gallery_2.mov" type="video/mp4">
                </video>
                <p class="has-text-centered">Zombie walk</p>
              </div>
              <div class="item item-bunny-headphones">
                <video poster="" id="bunny-headphones" autoplay muted loop playsinline height="100%">
                  <source src="./static/videos/website_gallery_3.mov" type="video/mp4">
                </video>
                <p class="has-text-centered">Sidesteps</p>
              </div>
            </div>
          </div>
        </div>
      </section>

      <hr class="divider" />
      <div class="columns is-centered has-text-centered bg">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Variation</h2>
          <div class="content has-text-justified">
            <p>
              Due to the stochastic nature of diffusion models, our method is able to generate variations using the same
              motion primer as input. We show four motions generated from a single primer, from left to right, we can
              see that the motions begin to differ significantly as time goes on.
            </p>
          </div>
          <div class="overview-image">
            <video poster="" id="bunny-headphones" controls autoplay muted loop playsinline height="100%">
              <source src="./static/videos/website_variation.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <hr class="divider" />

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Guided Generation</h2>
          <div class="content has-text-justified">
            <p>
              Given a set of <i>motion guides</i> (shown in yellow), we are able to perform them in sequence at desired
              points while generating plausible motion in the interactively generated frames (blue). Our method
              generates an entire motion sequence that contains the desired motion guides and the interactively
              synthesized motion. The interactively generated motions will “prepare and plan” for the upcoming motion
              guides.
            </p>
          </div>
          <div class="overview-image">
            <video poster="" id="bunny-headphones" controls autoplay muted loop playsinline height="100%">
              <source src="./static/videos/website_guided.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>

      <hr class="divider" />


      <div class="columns is-centered has-text-centered bg">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Trajectory Control</h2>
          <div class="content has-text-justified">
            <p>
              Our work can be applied to perform trajectory control during inference without additional training. 
              Similar to the mechanism of guided generation, trajectory control is achieved with inpainting in the motion buffer.
              Namely, during inference, we recursively overwrite the trjactory information in the motion buffer with frames in the desired trajectory.
            </p>
          </div>
          <div class="overview-image">
            <video poster="" id="bunny-headphones" controls autoplay muted loop playsinline height="100%">
              <source src="./static/videos/website_trajectory.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>

<!-- 
      <section class="section" id="acknowledgements">
        <div class="container has-text-justified">
          <h2 class="title">Acknowledgements</h2>
          <p>We thank the 3DL lab for their invaluable feedback and support. This work was supported in part through Uchicago's AI Cluster resources, services, and staff expertise.
          This work was also partially supported by the NSF under Grant No. 2241303, and a gift from Google Research.</p>
        </div>
      </section>

      <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code>@misc{zhang2023tedi,
              title={TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis}, 
              author={Zihan Zhang and Richard Liu and Kfir Aberman and Rana Hanocka},
              year={2023},
              eprint={2307.15042},
              archivePrefix={arXiv},
              primaryClass={cs.CV}
            }</code></pre>
        </div>
      </section> -->


      <footer class="footer">
        <div class="container">
          <div class="content has-text-centered">
            <a class="icon-link" href="https://github.com/threedle/" class="external-link" disabled>
              <i class="fab fa-github"></i>
            </a>
          </div>
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                <p>
                  This website is licensed under a <a rel="license"
                    href="https://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                </p>
                <p>
                  Parts of the code for this website are reused from the <a
                    href="https://github.com/nerfies/nerfies.github.io">source code</a> contributed by the authors of <a
                    href="https://nerfies.github.io/">Nerfies</a>.
                </p>
              </div>
            </div>
          </div>
        </div>
      </footer>

</body>

</html>